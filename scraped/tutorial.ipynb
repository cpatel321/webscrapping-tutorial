{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8f4844",
   "metadata": {},
   "source": [
    "\n",
    "# EM 615: Programming for Data Science\n",
    "**Indian Institute of Technology Gandhinagar**\n",
    "\n",
    "**Author**: [Chandrabhan Patel](https://www.linkedin.com/in/cpatel321/)  \n",
    "**Email**: [chandrabhan.patel@iitgn.ac.in](mailto:chandrabhan.patel@iitgn.ac.in)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Web Scraping\n",
    "Web scraping is the process of extracting data from websites. It allows you to collect structured data from the web, which can be used for analysis, visualization, or other applications in `data science`. In this tutorial, we'll cover the basics of web scraping using Python and the BeautifulSoup library.\n",
    "Also we will look at ethical considerations when scraping websites, along with overview of API's and how they can be used to extract data from websites.\n",
    "\n",
    "URL to sample website used in this tutorial: [https://cpatel321.github.io/webscrapping-tutorial/index.html](https://cpatel321.github.io/webscrapping-tutorial/index.html)\n",
    "\n",
    "### What is Web Scraping?\n",
    "Web scraping involves the following steps:\n",
    "1. Sending a request to a website.\n",
    "2. Retrieving the content of the webpage.\n",
    "3. Parsing the content to extract specific information.\n",
    "4. Saving the extracted data for further use.\n",
    "\n",
    "### Ethical Considerations\n",
    "##### robots.txt\n",
    "Before scraping, ensure the website permits it. Check the website's `robots.txt` file and adhere to its guidelines. \n",
    "More details about robots.txt can be found [here](https://www.robotstxt.org/robotstxt.html).\n",
    "\n",
    "1. Ensure you have the right to scrape the website's content. Some websites have terms of service that prohibit web scraping.\n",
    "\n",
    "2. Some websites have a creative commons license, which allows you to use the content for non-commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02c469",
   "metadata": {},
   "source": [
    "## How browsers work?\n",
    "\n",
    "When you visit a website, your browser sends a request to the server hosting the website. The server then sends back the content of the webpage, which is rendered by the browser. The browser interprets the HTML, CSS, and JavaScript code to display the webpage as intended.\n",
    "The protocol used for this communication is HTTP (HyperText Transfer Protocol) or HTTPS (HTTP Secure).\n",
    "## How search engines index websites?\n",
    "Search engines use web crawlers to index websites. These crawlers follow links on webpages to discover new content and index it in the search engine's database. When you search for a query, the search engine retrieves relevant results from its database and displays them to you.\n",
    "\n",
    "`sitemap.xml` file is used to tell search engines about the structure of the website and the URLs that should be indexed.\n",
    "\n",
    "### what are requests ?\n",
    "The `requests` library in Python allows you to send HTTP requests to websites and retrieve their content. You can use it to fetch webpages, download files, and interact with web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205b34e",
   "metadata": {},
   "source": [
    "# Setting Up the Environment for Scraping\n",
    "We'll use the following libraries in this tutorial:\n",
    "\n",
    "- `requests`: To send HTTP requests to fetch web pages.\n",
    "- `BeautifulSoup`: To parse and extract data from HTML content.\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63f1b6",
   "metadata": {},
   "source": [
    "# Step 1: Fetching the Webpage\n",
    "\n",
    "We'll begin by sending a GET request to the demo website and fetching its HTML content. The `requests` library is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce399470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <link rel=\"stylesheet\" href=\"style.css\">\n",
      "    <title>DataLand - Text Content</title>\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <h1>Random Text Content</h1>\n",
      "        <nav>\n",
      "            <ul>\n",
      "                <li><a href=\"index.html\">Home</a></li>\n",
      "                <li><a href=\"table.html\">Data Tables</a></li>\n",
      "                <li><a href=\"text.html\">Text Content</a></li>\n",
      "                <li><a href=\"figures.html\">Figures & Statistics</a></li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </header>\n",
      "    <section>\n",
      "        <article>\n",
      "            <h2>Article 1: Random Facts</h2>\n",
      "            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>\n",
      "            <p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 2: More Random Facts</h2>\n",
      "            <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.</p>\n",
      "            <p>Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 3: Fun Trivia</h2>\n",
      "            <p>Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.</p>\n",
      "            <p>Trivia can be a fun way to break the ice and learn something new every day!</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 4: Tech Tidbits</h2>\n",
      "            <p>The first computer virus, named \"Creeper,\" was created in 1971 as an experimental program to test self-replication.</p>\n",
      "            <p>Technology has come a long way since then, with both innovations and challenges continuing to shape our world.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 5: History Highlights</h2>\n",
      "            <p>The Great Wall of China stretches over 13,000 miles and was built to protect against invasions and raids from nomadic tribes.</p>\n",
      "            <p>Historical monuments like this remind us of the incredible engineering feats of the past.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 6: Space Wonders</h2>\n",
      "            <p>Jupiter, the largest planet in our solar system, has at least 79 moons. Its largest moon, Ganymede, is even bigger than the planet Mercury.</p>\n",
      "            <p>Space exploration continues to reveal fascinating mysteries of the universe.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 7: Amazing Nature</h2>\n",
      "            <p>Bananas are berries, but strawberries are not! This surprising fact is based on botanical definitions of fruit types.</p>\n",
      "            <p>Nature's quirks remind us of its diversity and complexity.</p>\n",
      "        </article>\n",
      "    </section>\n",
      "    <footer>\n",
      "        <p>DataLand 2024</p>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the demo website\n",
    "url = \"https://cpatel321.github.io/webscrapping-tutorial/text.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    print(response.text[:])  # Print the first 500 characters of the HTML\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235ad1c",
   "metadata": {},
   "source": [
    "# Step 2: Parsing the Webpage\n",
    "We'll use BeautifulSoup to parse the HTML content and extract meaningful data. In this case, we aim to retrieve the text content of all articles from the demo webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505713db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match ID 1: Article 1: Random Facts\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "\n",
      "Match ID 2: Article 2: More Random Facts\n",
      "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
      "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n",
      "\n",
      "Match ID 3: Article 3: Fun Trivia\n",
      "Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "Trivia can be a fun way to break the ice and learn something new every day!\n",
      "\n",
      "\n",
      "Match ID 4: Article 4: Tech Tidbits\n",
      "The first computer virus, named \"Creeper,\" was created in 1971 as an experimental program to test self-replication.\n",
      "Technology has come a long way since then, with both innovations and challenges continuing to shape our world.\n",
      "\n",
      "\n",
      "Match ID 5: Article 5: History Highlights\n",
      "The Great Wall of China stretches over 13,000 miles and was built to protect against invasions and raids from nomadic tribes.\n",
      "Historical monuments like this remind us of the incredible engineering feats of the past.\n",
      "\n",
      "\n",
      "Match ID 6: Article 6: Space Wonders\n",
      "Jupiter, the largest planet in our solar system, has at least 79 moons. Its largest moon, Ganymede, is even bigger than the planet Mercury.\n",
      "Space exploration continues to reveal fascinating mysteries of the universe.\n",
      "\n",
      "\n",
      "Match ID 7: Article 7: Amazing Nature\n",
      "Bananas are berries, but strawberries are not! This surprising fact is based on botanical definitions of fruit types.\n",
      "Nature's quirks remind us of its diversity and complexity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # parse the HTML content using BeautifulSoup\n",
    "\n",
    "articles = soup.find_all('article') # takes html tags as args, returns a list of all tags\n",
    "\n",
    "# Extract and print the text from each article\n",
    "for idx, article in enumerate(articles, start=1):\n",
    "    heading = article.find('h2').get_text(strip=True) # stripe justifies the text by removing leading and trailing whitespaces\n",
    "    content = article.find_all('p')\n",
    "    print(f\"Match ID {idx}: {heading}\")\n",
    "    for para in content:\n",
    "        print(para.get_text(strip=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f784e",
   "metadata": {},
   "source": [
    "# Step 3: Saving Data\n",
    "Extracted data can be saved to a file for further use. In this example, we'll save the article data to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1546786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to articles.txt\n"
     ]
    }
   ],
   "source": [
    "# saving articles to a text file\n",
    "with open(\"articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        heading = article.find('h2').get_text(strip=True)\n",
    "        content = article.find_all('p')\n",
    "        f.write(f\"Article {idx}: {heading}\\n\")\n",
    "        for para in content:\n",
    "            f.write(para.get_text(strip=True) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "print(\"Articles saved to articles.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013a84f",
   "metadata": {},
   "source": [
    "# Step 4: Scraping Tables\n",
    "If the webpage contains tables, they can be extracted and saved as CSV files for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6335878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Name', 'Age', 'Country']\n",
      "['1', 'John Doe', '30', 'USA']\n",
      "['2', 'Jane Smith', '25', 'Canada']\n",
      "\n",
      "Table data saved to table_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv #for parsing the data into csv and other type conversions\n",
    "\n",
    "#Here we will again send a request to a different webpage containing tables and extract the data from the tables\n",
    "table_url = \"https://cpatel321.github.io/webscrapping-tutorial/table.html\"\n",
    "soup_table = BeautifulSoup(requests.get(table_url).text, 'html.parser')\n",
    "\n",
    "tables = soup_table.find_all('table') # table is the tag used by most of the tables we see on websites\n",
    "\n",
    "if tables: # list is not empty\n",
    "    table = tables[0] # picking the first table \n",
    "    # print table data \n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Open a CSV file for writing\n",
    "    with open(\"table_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f) \n",
    "        \n",
    "        \n",
    "        #write each row to the CSV file\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            writer.writerow([cell.get_text(strip=True) for cell in cells])\n",
    "            print([cell.get_text(strip=True) for cell in cells])\n",
    "    print(\"\\nTable data saved to table_data.csv\")\n",
    "else:\n",
    "    print(\"No tables found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'images_and_text.csv'.\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin # for image url joining\n",
    "\n",
    "image_url = \"https://cpatel321.github.io/webscrapping-tutorial/figures.html\"\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup_image = BeautifulSoup(requests.get(image_url).text, 'html.parser')\n",
    "\n",
    "# Find all images and their associated text\n",
    "images = soup_image.find_all('img')\n",
    "text_data = soup_image.find_all(['p', 'ul'])\n",
    "\n",
    "# Create a CSV file to store the scraped data\n",
    "with open('images_and_text.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image path\", \"Description\"])  # Write the header\n",
    "\n",
    "    # Loop through each image and corresponding text\n",
    "    for image in images:\n",
    "        img_url = image['src']\n",
    "        # img_full_url = urljoin(image_url, img_url) # very useful function, saves a lot of manual work. used for joining the image url with the base url\n",
    "        # # print(\"Image URL:\", img_full_url)\n",
    "        # img_response = requests.get(img_full_url, stream=True)\n",
    "        # if img_response.status_code == 200:\n",
    "        #         img_name = img_full_url.split(\"/\")[-1]\n",
    "        #         with open(img_name, 'wb') as img_file:\n",
    "        #             for chunk in img_response.iter_content(1024):\n",
    "        #                 img_file.write(chunk)\n",
    "\n",
    "        # Find the associated description text (using next sibling or related paragraph)\n",
    "        associated_text = \"\"\n",
    "        for text in text_data:\n",
    "            if text.find_parent('section'):\n",
    "                associated_text = text.get_text(strip=True)\n",
    "                break  # Stop at the first relevant description\n",
    "\n",
    "        # Write the image URL and associated text to CSV\n",
    "        writer.writerow([img_url, associated_text])\n",
    "\n",
    "print(\"Data has been written to 'images_and_text.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86406b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'imdb_top_rated_movies.csv'.\n"
     ]
    }
   ],
   "source": [
    "# example with imdb website \n",
    "imdb_url = \"https://www.imdb.com/chart/top\"\n",
    "\n",
    "# Parse the HTML\n",
    "soup_imdb = BeautifulSoup(requests.get(imdb_url).text, 'html.parser')\n",
    "\n",
    "# Find the list of top-rated movies\n",
    "movies = soup_imdb.select('td.titleColumn')\n",
    "\n",
    "# Create a CSV file to store the scraped data\n",
    "\n",
    "with open('imdb_top_rated_movies.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Rank\", \"Title\", \"Year\", \"Rating\"])  # Write the header\n",
    "\n",
    "    # Loop through each movie\n",
    "    for movie in movies:\n",
    "        title = movie.find('a').get_text()\n",
    "        year = movie.find('span').get_text()[1:5]\n",
    "        rank = movie.get_text(strip=True).split('.')[0]\n",
    "        rating = movie.find_next('strong').get_text()\n",
    "        writer.writerow([rank, title, year, rating])\n",
    "\n",
    "print(\"Data has been written to 'imdb_top_rated_movies.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f37025",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "This tutorial demonstrated the basics of web scraping using Python. Remember to respect the terms of use of websites and avoid overloading servers with too many requests in a short time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
