{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8f4844",
   "metadata": {},
   "source": [
    "\n",
    "# EM 615: Programming for Data Science\n",
    "**Indian Institute of Technology Gandhinagar**\n",
    "\n",
    "**Author**: [Chandrabhan Patel](https://www.linkedin.com/in/cpatel321/)  \n",
    "**Email**: [chandrabhan.patel@iitgn.ac.in](mailto:chandrabhan.patel@iitgn.ac.in)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Web Scraping\n",
    "Web scraping is the process of extracting data from websites. It allows you to collect structured data from the web, which can be used for analysis, visualization, or other applications in `data science`. In this tutorial, we'll cover the basics of web scraping using Python and the BeautifulSoup library.\n",
    "Also we will look at ethical considerations when scraping websites, along with overview of API's and how they can be used to extract data from websites.\n",
    "\n",
    "URL to sample website used in this tutorial: [https://cpatel321.github.io/webscrapping-tutorial/index.html](https://cpatel321.github.io/webscrapping-tutorial/index.html)\n",
    "\n",
    "### What is Web Scraping?\n",
    "Web scraping involves the following steps:\n",
    "1. Sending a request to a website.\n",
    "2. Retrieving the content of the webpage.\n",
    "3. Parsing the content to extract specific information.\n",
    "4. Saving the extracted data for further use.\n",
    "\n",
    "### Ethical Considerations\n",
    "##### robots.txt\n",
    "Before scraping, ensure the website permits it. Check the website's `robots.txt` file and adhere to its guidelines. \n",
    "More details about robots.txt can be found [here](https://www.robotstxt.org/robotstxt.html).\n",
    "\n",
    "1. Ensure you have the right to scrape the website's content. Some websites have terms of service that prohibit web scraping.\n",
    "\n",
    "2. Some websites have a creative commons license, which allows you to use the content for non-commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02c469",
   "metadata": {},
   "source": [
    "## How browsers work?\n",
    "\n",
    "When you visit a website, your browser sends a request to the server hosting the website. The server then sends back the content of the webpage, which is rendered by the browser. The browser interprets the HTML, CSS, and JavaScript code to display the webpage as intended.\n",
    "The protocol used for this communication is HTTP (HyperText Transfer Protocol) or HTTPS (HTTP Secure).\n",
    "## How search engines index websites?\n",
    "Search engines use web crawlers to index websites. These crawlers follow links on webpages to discover new content and index it in the search engine's database. When you search for a query, the search engine retrieves relevant results from its database and displays them to you.\n",
    "\n",
    "`sitemap.xml` file is used to tell search engines about the structure of the website and the URLs that should be indexed.\n",
    "\n",
    "### what are requests ?\n",
    "The `requests` library in Python allows you to send HTTP requests to websites and retrieve their content. You can use it to fetch webpages, download files, and interact with web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205b34e",
   "metadata": {},
   "source": [
    "# Setting Up the Environment for Scraping\n",
    "We'll use the following libraries in this tutorial:\n",
    "\n",
    "- `requests`: To send HTTP requests to fetch web pages.\n",
    "- `BeautifulSoup`: To parse and extract data from HTML content.\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63f1b6",
   "metadata": {},
   "source": [
    "# Step 1: Fetching the Webpage\n",
    "\n",
    "We'll begin by sending a GET request to the demo website and fetching its HTML content. The `requests` library is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce399470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <link rel=\"stylesheet\" href=\"style.css\">\n",
      "    <title>DataLand - Text Content</title>\n",
      "    <link rel=\"icon\" href=\"https://iitgn.ac.in/assets/img/fevicon/favicon-96x96.png\" type=\"image/x-icon\">\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <h1>Random Text Content</h1>\n",
      "        <nav>\n",
      "            <ul>\n",
      "                <li><a href=\"index.html\">Home</a></li>\n",
      "                <li><a href=\"table.html\">Data Tables</a></li>\n",
      "                <li><a href=\"text.html\">Text Content</a></li>\n",
      "                <li><a href=\"figures.html\">Figures & Statistics</a></li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </header>\n",
      "    <section>\n",
      "        <article>\n",
      "            <h2>Article 1: Random Facts</h2>\n",
      "            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>\n",
      "            <p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 2: More Random Facts</h2>\n",
      "            <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.</p>\n",
      "            <p>Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 3: Fun Trivia</h2>\n",
      "            <p>Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.</p>\n",
      "            <p>Trivia can be a fun way to break the ice and learn something new every day!</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 4: Tech Tidbits</h2>\n",
      "            <p>The first computer virus, named \"Creeper,\" was created in 1971 as an experimental program to test self-replication.</p>\n",
      "            <p>Technology has come a long way since then, with both innovations and challenges continuing to shape our world.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 5: History Highlights</h2>\n",
      "            <p>The Great Wall of China stretches over 13,000 miles and was built to protect against invasions and raids from nomadic tribes.</p>\n",
      "            <p>Historical monuments like this remind us of the incredible engineering feats of the past.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 6: Space Wonders</h2>\n",
      "            <p>Jupiter, the largest planet in our solar system, has at least 79 moons. Its largest moon, Ganymede, is even bigger than the planet Mercury.</p>\n",
      "            <p>Space exploration continues to reveal fascinating mysteries of the universe.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 7: Amazing Nature</h2>\n",
      "            <p>Bananas are berries, but strawberries are not! This surprising fact is based on botanical definitions of fruit types.</p>\n",
      "            <p>Nature's quirks remind us of its diversity and complexity.</p>\n",
      "        </article>\n",
      "    </section>\n",
      "    <footer>\n",
      "        <p>DataLand 2024</p>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the demo website\n",
    "url = \"https://cpatel321.github.io/webscrapping-tutorial/text.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    print(response.text[:])  # Print the first 500 characters of the HTML\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235ad1c",
   "metadata": {},
   "source": [
    "# Step 2: Parsing the Webpage\n",
    "We'll use BeautifulSoup to parse the HTML content and extract meaningful data. In this case, we aim to retrieve the text content of all articles from the demo webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "505713db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match ID 1: Article 1: Random Facts\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "\n",
      "Match ID 2: Article 2: More Random Facts\n",
      "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
      "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n",
      "\n",
      "Match ID 3: Article 3: Fun Trivia\n",
      "Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "Trivia can be a fun way to break the ice and learn something new every day!\n",
      "\n",
      "\n",
      "Match ID 4: Article 4: Tech Tidbits\n",
      "The first computer virus, named \"Creeper,\" was created in 1971 as an experimental program to test self-replication.\n",
      "Technology has come a long way since then, with both innovations and challenges continuing to shape our world.\n",
      "\n",
      "\n",
      "Match ID 5: Article 5: History Highlights\n",
      "The Great Wall of China stretches over 13,000 miles and was built to protect against invasions and raids from nomadic tribes.\n",
      "Historical monuments like this remind us of the incredible engineering feats of the past.\n",
      "\n",
      "\n",
      "Match ID 6: Article 6: Space Wonders\n",
      "Jupiter, the largest planet in our solar system, has at least 79 moons. Its largest moon, Ganymede, is even bigger than the planet Mercury.\n",
      "Space exploration continues to reveal fascinating mysteries of the universe.\n",
      "\n",
      "\n",
      "Match ID 7: Article 7: Amazing Nature\n",
      "Bananas are berries, but strawberries are not! This surprising fact is based on botanical definitions of fruit types.\n",
      "Nature's quirks remind us of its diversity and complexity.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # parse the HTML content using BeautifulSoup\n",
    "\n",
    "articles = soup.find_all('article') # takes html tags as args, returns a list of all tags\n",
    "\n",
    "# Extract and print the text from each article\n",
    "for idx, article in enumerate(articles, start=1):\n",
    "    heading = article.find('h2').get_text(strip=True) # stripe justifies the text by removing leading and trailing whitespaces\n",
    "    content = article.find_all('p')\n",
    "    print(f\"Match ID {idx}: {heading}\")\n",
    "    for para in content:\n",
    "        print(para.get_text(strip=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f784e",
   "metadata": {},
   "source": [
    "# Step 3: Saving Data\n",
    "Extracted data can be saved to a file for further use. In this example, we'll save the article data to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1546786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to articles.txt\n"
     ]
    }
   ],
   "source": [
    "# saving articles to a text file\n",
    "with open(\"articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        heading = article.find('h2').get_text(strip=True)\n",
    "        content = article.find_all('p')\n",
    "        f.write(f\"Article {idx}: {heading}\\n\")\n",
    "        for para in content:\n",
    "            f.write(para.get_text(strip=True) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "print(\"Articles saved to articles.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013a84f",
   "metadata": {},
   "source": [
    "# Step 4: Scraping Tables\n",
    "If the webpage contains tables, they can be extracted and saved as CSV files for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6335878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Row', 'Column A', 'Column B', 'Column C', 'Column D']\n",
      "['1', 'Data A1', 'Data B1', 'Data C1', 'Data D1']\n",
      "['2', 'Data A2', 'Data B2', 'Data C2', 'Data D2']\n",
      "['3', 'Data A3', 'Data B3', 'Data C3', 'Data D3']\n",
      "['4', 'Data A4', 'Data B4', 'Data C4', 'Data D4']\n",
      "['5', 'Data A5', 'Data B5', 'Data C5', 'Data D5']\n",
      "['6', 'Data A6', 'Data B6', 'Data C6', 'Data D6']\n",
      "['7', 'Data A7', 'Data B7', 'Data C7', 'Data D7']\n",
      "['8', 'Data A8', 'Data B8', 'Data C8', 'Data D8']\n",
      "['9', 'Data A9', 'Data B9', 'Data C9', 'Data D9']\n",
      "['10', 'Data A10', 'Data B10', 'Data C10', 'Data D10']\n",
      "\n",
      "Table data saved to table_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv #for parsing the data into csv and other type conversions\n",
    "\n",
    "#Here we will again send a request to a different webpage containing tables and extract the data from the tables\n",
    "table_url = \"https://cpatel321.github.io/webscrapping-tutorial/table.html\"\n",
    "soup_table = BeautifulSoup(requests.get(table_url).text, 'html.parser')\n",
    "\n",
    "tables = soup_table.find_all('table') # table is the tag used by most of the tables we see on websites\n",
    "\n",
    "if tables: # list is not empty\n",
    "    table = tables[2] # picking the first table \n",
    "    # print table data \n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Open a CSV file for writing\n",
    "    with open(\"table_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f) \n",
    "        \n",
    "        \n",
    "        #write each row to the CSV file\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            writer.writerow([cell.get_text(strip=True) for cell in cells])\n",
    "            print([cell.get_text(strip=True) for cell in cells])\n",
    "    print(\"\\nTable data saved to table_data.csv\")\n",
    "else:\n",
    "    print(\"No tables found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa4c5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'images_and_text.csv'.\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin # for image url joining\n",
    "\n",
    "image_url = \"https://cpatel321.github.io/webscrapping-tutorial/figures.html\"\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup_image = BeautifulSoup(requests.get(image_url).text, 'html.parser')\n",
    "\n",
    "# Find all images and their associated text\n",
    "images = soup_image.find_all('img')\n",
    "text_data = soup_image.find_all(['p', 'ul'])\n",
    "\n",
    "# Create a CSV file to store the scraped data\n",
    "with open('images_and_text.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image path\", \"Description\"])  # Write the header\n",
    "\n",
    "    # Loop through each image and corresponding text\n",
    "    for image in images:\n",
    "        img_url = image['src']\n",
    "        # img_full_url = urljoin(image_url, img_url) # very useful function, saves a lot of manual work. used for joining the image url with the base url\n",
    "        # # print(\"Image URL:\", img_full_url)\n",
    "        # img_response = requests.get(img_full_url, stream=True)\n",
    "        # if img_response.status_code == 200:\n",
    "        #         img_name = img_full_url.split(\"/\")[-1]\n",
    "        #         with open(img_name, 'wb') as img_file:\n",
    "        #             for chunk in img_response.iter_content(1024):\n",
    "        #                 img_file.write(chunk)\n",
    "\n",
    "        # Find the associated description text (using next sibling or related paragraph)\n",
    "        associated_text = \"\"\n",
    "        for text in text_data:\n",
    "            if text.find_parent('section'):\n",
    "                associated_text = text.get_text(strip=True)\n",
    "                break  # Stop at the first relevant description\n",
    "\n",
    "        # Write the image URL and associated text to CSV\n",
    "        writer.writerow([img_url, associated_text])\n",
    "\n",
    "print(\"Data has been written to 'images_and_text.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9930a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The Shawshank Redemption 9.3 1994\n",
      "2. The Godfather 9.2 1972\n",
      "3. The Dark Knight 9.0 2008\n",
      "4. The Godfather: Part II 9.0 1974\n",
      "5. Schindler's List 9.0 1993\n",
      "6. The Lord of the Rings: The Return of the King 9.0 2003\n",
      "7. 12 Angry Men 9.0 1957\n",
      "8. The Lord of the Rings: The Fellowship of the Ring 8.9 2001\n",
      "9. Pulp Fiction 8.9 1994\n",
      "10. Inception 8.8 2010\n",
      "11. Fight Club 8.8 1999\n",
      "12. Forrest Gump 8.8 1994\n",
      "13. The Lord of the Rings: The Two Towers 8.8 2002\n",
      "14. Il Buono, Il Brutto, Il Cattivo 8.8 1966\n",
      "15. 12th Fail 8.8 2023\n",
      "16. Interstellar 8.7 2014\n",
      "17. GoodFellas 8.7 1990\n",
      "18. The Matrix 8.7 1999\n",
      "19. One Flew Over the Cuckoo's Nest 8.7 1975\n",
      "20. Star Wars: Episode V - The Empire Strikes Back 8.7 1980\n"
     ]
    }
   ],
   "source": [
    "\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}\n",
    "\n",
    "imdb_url = \"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\"\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup_imdb = BeautifulSoup(requests.get(imdb_url, headers=header).text, 'html.parser')\n",
    "# print(soup_imdb)\n",
    "\n",
    "movies = soup_imdb.find_all('li',class_='ipc-metadata-list-summary-item')\n",
    "# print(movies[0].prettify())\n",
    "\n",
    "# # Loop through each movie and extract relevant details\n",
    "with open('imdb.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Name', 'Rating', 'Year'])\n",
    "        for movie in movies[0:20]:\n",
    "            # Extract movie name\n",
    "            name = movie.find('h3', class_='ipc-title__text').get_text(strip=True)\n",
    "            rating = movie.find('span', class_='ipc-rating-star--rating').get_text(strip=True)\n",
    "            year = movie.find('span', class_='dli-title-metadata-item').get_text(strip=True)\n",
    "            print(name, rating, year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0406adb",
   "metadata": {},
   "source": [
    "\n",
    "## API and Web Scraping Tutorial\n",
    "\n",
    "### Introduction to APIs\n",
    "\n",
    "APIs (Application Programming Interfaces) allow different software applications to communicate with each other by defining specific methods and rules for interaction. When it comes to web scraping, APIs are a powerful tool that allows you to programmatically interact with web servers and extract data efficiently.\n",
    "\n",
    "In simple terms, APIs let us retrieve data from web services without needing to manually scrape HTML pages.\n",
    "\n",
    "### Why use APIs for web scraping?\n",
    "- **Efficiency:** APIs provide data in a structured format (e.g., JSON, XML), making it easier to extract relevant information.\n",
    "- **Legality:** Some websites offer official APIs, ensuring that scraping is done within the legal boundaries.\n",
    "- **Rate Limiting and Authentication:** APIs can be used in a more controlled manner, reducing the chances of being blocked for excessive scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10c566b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': {'updated': 'Nov 21, 2024 16:57:48 UTC', 'updatedISO': '2024-11-21T16:57:48+00:00', 'updateduk': 'Nov 21, 2024 at 16:57 GMT'}, 'disclaimer': 'This data was produced from the CoinDesk Bitcoin Price Index (USD). Non-USD currency data converted using hourly conversion rate from openexchangerates.org', 'chartName': 'Bitcoin', 'bpi': {'USD': {'code': 'USD', 'symbol': '&#36;', 'rate': '97,310.834', 'description': 'United States Dollar', 'rate_float': 97310.8338}, 'GBP': {'code': 'GBP', 'symbol': '&pound;', 'rate': '77,120.101', 'description': 'British Pound Sterling', 'rate_float': 77120.1008}, 'EUR': {'code': 'EUR', 'symbol': '&euro;', 'rate': '92,586.685', 'description': 'Euro', 'rate_float': 92586.6847}}}\n",
      "Bitcoin price in USD: 97,310.834\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the API\n",
    "api_url = 'https://api.coindesk.com/v1/bpi/currentprice.json'\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(api_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Convert JSON response into a dictionary\n",
    "    print(data)\n",
    "else:\n",
    "    print('Error:', response.status_code)\n",
    "\n",
    "# Extract the Bitcoin price in USD\n",
    "usd_price = data['bpi']['USD']['rate']\n",
    "print(f\"Bitcoin price in USD: {usd_price}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f37025",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "This tutorial demonstrated the basics of web scraping using Python. Remember to respect the terms of use of websites and avoid overloading servers with too many requests in a short time.\n",
    "Referenecs: [https://www.tutorialspoint.com/python_web_scraping/legality_of_python_web_scraping.htm](https://www.tutorialspoint.com/python_web_scraping/legality_of_python_web_scraping.htm)\n",
    "\n",
    "Resources related to free APIs:\n",
    "1. [https://publicapi.dev/](https://publicapi.dev/)\n",
    "2. [https://github.com/public-apis/public-apis](https://github.com/public-apis/public-apis)\n",
    "\n",
    "Resources related to Captcha bypassing:\n",
    "1. [https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_processing_captcha.htm](https://www.tutorialspoint.com/python_web_scraping/python_web_scraping_processing_captcha.htm)\n",
    "\n",
    "License: Creative Commons "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
