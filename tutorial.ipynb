{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping\n",
    "\n",
    "Web scraping is the process of extracting data from websites. It allows you to collect structured data from the web, which can be used for analysis, visualization, or other applications in `data science`. In this tutorial, we'll cover the basics of web scraping using Python and the BeautifulSoup library.\n",
    "Also we will look at ethical considerations when scraping websites, along with overview of API's and how they can be used to extract data from websites.\n",
    "\n",
    "### What is Web Scraping?\n",
    "Web scraping involves the following steps:\n",
    "1. Sending a request to a website.\n",
    "2. Retrieving the content of the webpage.\n",
    "3. Parsing the content to extract specific information.\n",
    "4. Saving the extracted data for further use.\n",
    "\n",
    "### Ethical Considerations\n",
    "Before scraping, ensure the website permits it. Check the website's `robots.txt` file and adhere to its guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Environment\n",
    "We'll use the following libraries in this tutorial:\n",
    "\n",
    "- `requests`: To send HTTP requests to fetch web pages.\n",
    "- `BeautifulSoup`: To parse and extract data from HTML content.\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Fetching the Webpage\n",
    "\n",
    "We'll begin by sending a GET request to the demo website and fetching its HTML content. The `requests` library is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <link rel=\"stylesheet\" href=\"style.css\">\n",
      "    <title>DataLand - Text Content</title>\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <h1>Random Text Content</h1>\n",
      "        <nav>\n",
      "            <ul>\n",
      "                <li><a href=\"index.html\">Home</a></li>\n",
      "                <li><a href=\"table.html\">Data Tables</a></li>\n",
      "                <li><a href=\"text.html\">Text Content</a></li>\n",
      "                <li><a href=\"figures.html\">Figures & Statistics</a></li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </header>\n",
      "    <section>\n",
      "        <article>\n",
      "            <h2>Article 1: Random Facts</h2>\n",
      "            <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>\n",
      "            <p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.</p>\n",
      "        </article>\n",
      "\n",
      "        <article>\n",
      "            <h2>Article 2: More Random Facts</h2>\n",
      "            <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.</p>\n",
      "            <p>Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n",
      "        </article>\n",
      "    </section>\n",
      "    <footer>\n",
      "        <p>&copy; 2024 DataLand</p>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the demo website\n",
    "url = \"https://cpatel321.github.io/webscrapping-tutorial/text.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    print(response.text[:])  # Print the first 500 characters of the HTML\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Parsing the Webpage\n",
    "We'll use BeautifulSoup to parse the HTML content and extract meaningful data. In this case, we aim to retrieve the text content of all articles from the demo webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match ID 1: Article 1: Random Facts\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "\n",
      "Match ID 2: Article 2: More Random Facts\n",
      "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
      "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # parse the HTML content using BeautifulSoup\n",
    "\n",
    "articles = soup.find_all('article') # takes html tags as args, returns a list of all tags\n",
    "\n",
    "# Extract and print the text from each article\n",
    "for idx, article in enumerate(articles, start=1):\n",
    "    heading = article.find('h2').get_text(strip=True) # stripe justifies the text by removing leading and trailing whitespaces\n",
    "    content = article.find_all('p')\n",
    "    print(f\"Match ID {idx}: {heading}\")\n",
    "    for para in content:\n",
    "        print(para.get_text(strip=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Saving Data\n",
    "Extracted data can be saved to a file for further use. In this example, we'll save the article data to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles saved to articles.txt\n"
     ]
    }
   ],
   "source": [
    "# saving articles to a text file\n",
    "with open(\"articles.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, article in enumerate(articles, start=1):\n",
    "        heading = article.find('h2').get_text(strip=True)\n",
    "        content = article.find_all('p')\n",
    "        f.write(f\"Article {idx}: {heading}\\n\")\n",
    "        for para in content:\n",
    "            f.write(para.get_text(strip=True) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "print(\"Articles saved to articles.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Scraping Tables\n",
    "If the webpage contains tables, they can be extracted and saved as CSV files for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Name', 'Age', 'Country']\n",
      "['1', 'John Doe', '30', 'USA']\n",
      "['2', 'Jane Smith', '25', 'Canada']\n",
      "\n",
      "Table data saved to table_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv #for parsing the data into csv and other type conversions\n",
    "\n",
    "#Here we will again send a request to a different webpage containing tables and extract the data from the tables\n",
    "table_url = \"https://cpatel321.github.io/webscrapping-tutorial/table.html\"\n",
    "soup_table = BeautifulSoup(requests.get(table_url).text, 'html.parser')\n",
    "\n",
    "tables = soup_table.find_all('table') # table is the tag used by most of the tables we see on websites\n",
    "\n",
    "if tables: # list is not empty\n",
    "    table = tables[0] # picking the first table \n",
    "    # print table data \n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Open a CSV file for writing\n",
    "    with open(\"table_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f) \n",
    "        \n",
    "        \n",
    "        #write each row to the CSV file\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            writer.writerow([cell.get_text(strip=True) for cell in cells])\n",
    "            print([cell.get_text(strip=True) for cell in cells])\n",
    "    print(\"\\nTable data saved to table_data.csv\")\n",
    "else:\n",
    "    print(\"No tables found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'images_and_text.csv'.\n"
     ]
    }
   ],
   "source": [
    "import urljoin\n",
    "image_url = \"https://cpatel321.github.io/webscrapping-tutorial/figures.html\"\n",
    "\n",
    "\n",
    "# Parse the HTML\n",
    "soup_image = BeautifulSoup(requests.get(image_url).text, 'html.parser')\n",
    "\n",
    "# Find all images and their associated text\n",
    "images = soup_image.find_all('img')\n",
    "text_data = soup_image.find_all(['p', 'ul'])\n",
    "\n",
    "# Create a CSV file to store the scraped data\n",
    "with open('images_and_text.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Image URL\", \"Description\"])  # Write the header\n",
    "\n",
    "    # Loop through each image and corresponding text\n",
    "    for image in images:\n",
    "        img_url = image['src']\n",
    "        # full url of image to dowload it\n",
    "        img_url = urljoin(image_url, img_url)\n",
    "        # saving the image in scrapped folder\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(f\"image_{images.index(image)}.jpg\", 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "        # img_alt = image['alt']\n",
    "        \n",
    "        # Find the associated description text (using next sibling or related paragraph)\n",
    "        associated_text = \"\"\n",
    "        for text in text_data:\n",
    "            if text.find_parent('section'):\n",
    "                associated_text = text.get_text(strip=True)\n",
    "                break  # Stop at the first relevant description\n",
    "\n",
    "        # Write the image URL and associated text to CSV\n",
    "        writer.writerow([img_url, associated_text])\n",
    "\n",
    "print(\"Data has been written to 'images_and_text.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes\n",
    "\n",
    "This tutorial demonstrated the basics of web scraping using Python. Remember to respect the terms of use of websites and avoid overloading servers with too many requests in a short time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
